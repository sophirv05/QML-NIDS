{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8196fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after preprocessing: (4463753, 6)\n",
      "                            duration  packets_count  fwd_packets_count  \\\n",
      "timestamp                                                                \n",
      "2018-02-23 07:15:39.588247  0.457770           18.0               10.0   \n",
      "2018-02-23 07:15:41.110103  0.160555           20.0               11.0   \n",
      "2018-02-23 07:15:41.111346  0.159061           20.0               11.0   \n",
      "2018-02-23 07:15:41.111651  0.158567           19.0               10.0   \n",
      "2018-02-23 07:15:41.111177  0.159373           20.0               11.0   \n",
      "\n",
      "                            bwd_packets_count  total_payload_bytes  \\\n",
      "timestamp                                                            \n",
      "2018-02-23 07:15:39.588247                8.0               4364.0   \n",
      "2018-02-23 07:15:41.110103                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111346                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111651                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111177                9.0               8099.0   \n",
      "\n",
      "                              bytes_rate  \n",
      "timestamp                                 \n",
      "2018-02-23 07:15:39.588247   9533.174372  \n",
      "2018-02-23 07:15:41.110103  50443.734114  \n",
      "2018-02-23 07:15:41.111346  50917.586894  \n",
      "2018-02-23 07:15:41.111651  51076.216769  \n",
      "2018-02-23 07:15:41.111177  50817.878278  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Classical Persistent Homology on Network Intrusion Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gudhi as gd\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Load Dataset\n",
    "# ------------------------------\n",
    "\n",
    "def load_and_preprocess_data(csv_file, features, timestamp_col='timestamp'):\n",
    "    \"\"\"Load and preprocess the CSV file.\"\"\"\n",
    "    # Read CSV using pandas directly for easier debugging and speed\n",
    "    df = pd.read_csv(\n",
    "        csv_file,\n",
    "        usecols=features,\n",
    "        dtype={col: 'float64' for col in features if col != timestamp_col}\n",
    "    )\n",
    "    \n",
    "    # Parse timestamp\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], errors='coerce')\n",
    "    df = df.dropna(subset=[timestamp_col])\n",
    "    df = df.set_index(timestamp_col)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Feature Selection & Scaling\n",
    "# ------------------------------\n",
    "\n",
    "def extract_point_cloud(df, feature_cols):\n",
    "    \"\"\"Extract and scale features for point cloud.\"\"\"\n",
    "    point_data = df[feature_cols].fillna(0).values\n",
    "    scaler = StandardScaler()\n",
    "    point_data_scaled = scaler.fit_transform(point_data)\n",
    "    return point_data_scaled\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Build Rips Complex & Persistence\n",
    "# ------------------------------\n",
    "\n",
    "def compute_persistent_homology(point_data_scaled, max_dimension=2, max_edge_length=3.0):\n",
    "    \"\"\"Build Rips Complex and compute persistent homology.\"\"\"\n",
    "    rips_complex = gd.RipsComplex(points=point_data_scaled, max_edge_length=max_edge_length)\n",
    "    simplex_tree = rips_complex.create_simplex_tree(max_dimension=max_dimension)\n",
    "    simplex_tree.compute_persistence()\n",
    "    return simplex_tree\n",
    "\n",
    "def plot_persistence(simplex_tree):\n",
    "    \"\"\"Plot the persistence diagram and barcode.\"\"\"\n",
    "    gd.plot_persistence_diagram(simplex_tree.persistence())\n",
    "    plt.title(\"Persistence Diagram\")\n",
    "    plt.show()\n",
    "    \n",
    "    gd.plot_persistence_barcode(simplex_tree.persistence())\n",
    "    plt.title(\"Persistence Barcode\")\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Betti Numbers Extraction\n",
    "# ------------------------------\n",
    "\n",
    "def extract_betti_numbers(simplex_tree):\n",
    "    \"\"\"Extract Betti numbers at max filtration scale.\"\"\"\n",
    "    betti_nums = simplex_tree.betti_numbers()\n",
    "    return betti_nums\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Graph Construction\n",
    "# ------------------------------\n",
    "\n",
    "def build_graph_from_point_cloud(point_data_scaled, threshold=1.5):\n",
    "    \"\"\"Build a graph from point cloud using distance threshold.\"\"\"\n",
    "    dist_mat = distance_matrix(point_data_scaled, point_data_scaled)\n",
    "    G = nx.Graph()\n",
    "    n_points = len(point_data_scaled)\n",
    "    G.add_nodes_from(range(n_points))\n",
    "    \n",
    "    # Efficient vectorized thresholding\n",
    "    rows, cols = np.where(np.triu(dist_mat, k=1) <= threshold)\n",
    "    edges = [(int(i), int(j), {'weight': dist_mat[i, j]}) for i, j in zip(rows, cols)]\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Visualization of Graph Metrics and Betti Numbers\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_graph_metrics(betti_nums, G):\n",
    "    \"\"\"Visualize Betti numbers and graph metrics.\"\"\"\n",
    "    degrees = np.array([deg for _, deg in G.degree()])\n",
    "    max_degree = degrees.max() if len(degrees) > 0 else 0\n",
    "    num_edges = G.number_of_edges()\n",
    "    \n",
    "    # Store in DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'beta_0': [betti_nums[0] if len(betti_nums) > 0 else 0],\n",
    "        'beta_1': [betti_nums[1] if len(betti_nums) > 1 else 0],\n",
    "        'beta_2': [betti_nums[2] if len(betti_nums) > 2 else 0],\n",
    "        'max_degree': [max_degree],\n",
    "        'num_edges': [num_edges]\n",
    "    })\n",
    "    \n",
    "    # Normalize\n",
    "    df_plot = summary_df.copy()\n",
    "    for col in df_plot.columns:\n",
    "        col_min, col_max = df_plot[col].min(), df_plot[col].max()\n",
    "        if col_max != col_min:\n",
    "            df_plot[col] = (df_plot[col] - col_min) / (col_max - col_min)\n",
    "        else:\n",
    "            df_plot[col] = 0.0\n",
    "    \n",
    "    df_plot.plot(figsize=(12, 6), title=\"Normalized Betti Numbers and Graph Metrics\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Main Pipeline\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    csv_file = \"mergedBFXSS.csv\"\n",
    "    features = [\n",
    "        'timestamp',\n",
    "        'duration',\n",
    "        'total_payload_bytes',\n",
    "        'packets_count',\n",
    "        'fwd_packets_count',\n",
    "        'bwd_packets_count',\n",
    "        'bytes_rate'\n",
    "    ]\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = load_and_preprocess_data(csv_file, features)\n",
    "    print(f\"Data shape after preprocessing: {df.shape}\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Extract and scale point cloud\n",
    "    point_data_scaled = extract_point_cloud(df, features[1:])\n",
    "    print(f\"Point cloud shape: {point_data_scaled.shape}\")\n",
    "    \n",
    "    # Build Rips Complex and compute PH\n",
    "    print(\"Building Rips Complex and computing persistence...\")\n",
    "    simplex_tree = compute_persistent_homology(point_data_scaled)\n",
    "    print(f\"Number of simplices: {simplex_tree.num_simplices()}\")\n",
    "    \n",
    "    # Plot persistence diagrams\n",
    "    plot_persistence(simplex_tree)\n",
    "    \n",
    "    # Extract Betti numbers\n",
    "    betti_nums = extract_betti_numbers(simplex_tree)\n",
    "    print(f\"Betti numbers: {betti_nums}\")\n",
    "    \n",
    "    # Build Graph\n",
    "    print(\"Building Graph from point cloud...\")\n",
    "    G = build_graph_from_point_cloud(point_data_scaled, threshold=1.5)\n",
    "    print(f\"Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    \n",
    "    # Visualize metrics\n",
    "    summary_df = visualize_graph_metrics(betti_nums, G)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(summary_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d38d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Persistent Homology on Network Intrusion Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gudhi as gd\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance_matrix\n",
    "from tqdm import tqdm\n",
    "import dask.dataframe as dd \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3e629e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to Pandas DataFrame...\n",
      "                            duration  packets_count  fwd_packets_count  \\\n",
      "timestamp                                                                \n",
      "2018-02-23 07:15:39.588247  0.457770           18.0               10.0   \n",
      "2018-02-23 07:15:41.110103  0.160555           20.0               11.0   \n",
      "2018-02-23 07:15:41.111346  0.159061           20.0               11.0   \n",
      "2018-02-23 07:15:41.111651  0.158567           19.0               10.0   \n",
      "2018-02-23 07:15:41.111177  0.159373           20.0               11.0   \n",
      "\n",
      "                            bwd_packets_count  total_payload_bytes  \\\n",
      "timestamp                                                            \n",
      "2018-02-23 07:15:39.588247                8.0               4364.0   \n",
      "2018-02-23 07:15:41.110103                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111346                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111651                9.0               8099.0   \n",
      "2018-02-23 07:15:41.111177                9.0               8099.0   \n",
      "\n",
      "                              bytes_rate  \n",
      "timestamp                                 \n",
      "2018-02-23 07:15:39.588247   9533.174372  \n",
      "2018-02-23 07:15:41.110103  50443.734114  \n",
      "2018-02-23 07:15:41.111346  50917.586894  \n",
      "2018-02-23 07:15:41.111651  51076.216769  \n",
      "2018-02-23 07:15:41.111177  50817.878278  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# 1. Load Dataset\n",
    "# ------------------------------\n",
    "\n",
    "features = [\n",
    "    'timestamp',\n",
    "    'duration',\n",
    "    'total_payload_bytes',\n",
    "    'packets_count',\n",
    "    'fwd_packets_count',\n",
    "    'bwd_packets_count',\n",
    "    'bytes_rate'\n",
    "]\n",
    "\n",
    "# Load with Dask\n",
    "ddf = dd.read_csv(\n",
    "    \"mergedBFXSS.csv\",\n",
    "    usecols=features,\n",
    "    assume_missing=True,\n",
    "    dtype={\n",
    "        'timestamp': 'object',\n",
    "        'duration': 'float64',\n",
    "        'total_payload_bytes': 'float64',\n",
    "        'packets_count': 'float64',\n",
    "        'fwd_packets_count': 'float64',\n",
    "        'bwd_packets_count': 'float64',\n",
    "        'bytes_rate': 'float64'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert Dask to Pandas **before** timestamp parsing\n",
    "print(\"Converting to Pandas DataFrame...\")\n",
    "df = ddf.compute()\n",
    "\n",
    "# Parse timestamp and drop NaT rows\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Set timestamp as index\n",
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83cdf376",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Parse timestamp and drop rows with NaT\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ddf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mddf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m ddf \u001b[38;5;241m=\u001b[39m ddf\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/dask/dataframe/dask_expr/_collection.py:425\u001b[0m, in \u001b[0;36mFrameBase.__getitem__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, np\u001b[38;5;241m.\u001b[39mgeneric):\n\u001b[1;32m    424\u001b[0m     other \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnew_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/dask/_collections.py:8\u001b[0m, in \u001b[0;36mnew_collection\u001b[0;34m(expr)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_collection\u001b[39m(expr):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create new collection from an expr\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mexpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[1;32m      9\u001b[0m     expr\u001b[38;5;241m.\u001b[39m_name  \u001b[38;5;66;03m# Ensure backend is imported\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_collection_type(meta)(expr)\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/dask/dataframe/dask_expr/_expr.py:2146\u001b[0m, in \u001b[0;36mProjection._meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m   2144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39m_meta):\n\u001b[0;32m-> 2146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_meta\u001b[49m\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;66;03m# if we are not a DataFrame and have a scalar, we reduce to a scalar\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mslice\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[1;32m   2149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperand(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2150\u001b[0m     ):\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/dask/dataframe/dask_expr/_expr.py:561\u001b[0m, in \u001b[0;36mBlockwise._meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     args \u001b[38;5;241m=\u001b[39m [op\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, Expr) \u001b[38;5;28;01melse\u001b[39;00m op \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args]\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "# Parse timestamp and drop rows with NaT\n",
    "ddf[\"timestamp\"] = dd.to_datetime(ddf[\"timestamp\"], errors=\"coerce\")\n",
    "ddf = ddf.dropna(subset=['timestamp'])\n",
    "ddf = ddf.set_index(\"timestamp\")\n",
    "\n",
    "# Trigger computation\n",
    "df = ddf.compute()\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c65f06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2. Feature Selection\n",
    "# ------------------------------\n",
    "\n",
    "# Extract only numeric features (skip 'timestamp' index)\n",
    "point_data = df[features[1:]].fillna(0).values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "point_data_scaled = scaler.fit_transform(point_data)\n",
    "\n",
    "print(f\"Shape of point cloud: {point_data_scaled.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Build Rips Complex & Filtration\n",
    "# ------------------------------\n",
    "\n",
    "max_dimension = 2\n",
    "max_edge_length = 3.0  # adjust as needed\n",
    "\n",
    "print(\"Building Rips Complex...\")\n",
    "rips_complex = gd.RipsComplex(points=point_data_scaled, max_edge_length=max_edge_length)\n",
    "simplex_tree = rips_complex.create_simplex_tree(max_dimension=max_dimension)\n",
    "\n",
    "print(f\"Number of simplices: {simplex_tree.num_simplices()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Compute Persistence Diagram\n",
    "# ------------------------------\n",
    "\n",
    "print(\"Computing Persistence...\")\n",
    "with tqdm(total=1) as pbar:\n",
    "    persistence = simplex_tree.persistence()\n",
    "    pbar.update(1)\n",
    "\n",
    "gd.plot_persistence_diagram(persistence)\n",
    "plt.title(\"Persistence Diagram\")\n",
    "plt.show()\n",
    "\n",
    "gd.plot_persistence_barcode(persistence)\n",
    "plt.title(\"Persistence Barcode\")\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Extract Betti Numbers\n",
    "# ------------------------------\n",
    "\n",
    "betti_nums = simplex_tree.betti_numbers()\n",
    "print(f\"Betti numbers at max filtration scale: {betti_nums}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Build Graph (for Network Intrusion Detection)\n",
    "# ------------------------------\n",
    "\n",
    "print(\"Building Graph from point cloud...\")\n",
    "graph_threshold = 1.5  # adjust as needed\n",
    "G = nx.Graph()\n",
    "dist_mat = distance_matrix(point_data_scaled, point_data_scaled)\n",
    "for i in tqdm(range(len(point_data_scaled)), desc=\"Adding edges\"):\n",
    "    for j in range(i+1, len(point_data_scaled)):\n",
    "        dist = dist_mat[i, j]\n",
    "        if dist <= graph_threshold:\n",
    "            G.add_edge(i, j, weight=dist)\n",
    "\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Visualize Graph Metrics and Betti Numbers\n",
    "# ------------------------------\n",
    "\n",
    "degrees = np.array([deg for _, deg in G.degree()])\n",
    "max_degree = degrees.max()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'beta_0': [betti_nums[0] if len(betti_nums) > 0 else 0],\n",
    "    'beta_1': [betti_nums[1] if len(betti_nums) > 1 else 0],\n",
    "    'beta_2': [betti_nums[2] if len(betti_nums) > 2 else 0],\n",
    "    'max_degree': [max_degree],\n",
    "    'num_edges': [num_edges]\n",
    "})\n",
    "\n",
    "# Normalize for visualization\n",
    "df_plot = summary_df.copy()\n",
    "for col in ['beta_0', 'beta_1', 'beta_2', 'max_degree', 'num_edges']:\n",
    "    df_plot[col] = (df_plot[col] - df_plot[col].min()) / (df_plot[col].max() - df_plot[col].min() + 1e-9)\n",
    "\n",
    "df_plot.plot(figsize=(12, 6), title=\"Normalized Betti Numbers and Graph Metrics\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Summary\n",
    "# ------------------------------\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Betti_0 (Connected Components): {betti_nums[0] if len(betti_nums) > 0 else 'N/A'}\")\n",
    "print(f\"Betti_1 (1-dimensional Holes): {betti_nums[1] if len(betti_nums) > 1 else 'N/A'}\")\n",
    "print(f\"Betti_2 (2-dimensional Holes): {betti_nums[2] if len(betti_nums) > 2 else 'N/A'}\")\n",
    "print(f\"Max Degree in Graph: {max_degree}\")\n",
    "print(f\"Number of Edges in Graph: {num_edges}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
