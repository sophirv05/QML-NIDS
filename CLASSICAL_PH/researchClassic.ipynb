{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4acc3e19",
   "metadata": {},
   "source": [
    "# Yet another classical approach - this time based on Ziqi Yuan et al's \"PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology Optimization\"-https://arxiv.org/pdf/2401.10547\n",
    "\n",
    "Constructs graphs with multi-feature edges, calculates Betti number statistics, + trains model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6db83f",
   "metadata": {},
   "source": [
    "## DATABASE PREP\n",
    "Code for reading in concatenated csv (benign + attack), sorting it by timestamp (thus combining the data as before extraction), retaining only relevant features/columns, and creating new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb927669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd          # large‑CSV engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from gudhi import SimplexTree\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------- parameters ----------\n",
    "CSV_PATH   = \"mergedBFXSS.csv\"   # or bot.csv later\n",
    "TIME_COL   = \"timestamp\"\n",
    "SRC, DST   = \"src_ip\", \"dst_ip\"\n",
    "LABEL = \"label\"\n",
    "WEIGHT_COL = \"total_payload_bytes\"            # can change\n",
    "WIN        = \"5min\"                           # resample size\n",
    "VR_EPS     = 2000                             # distance/weight threshold\n",
    "MAX_DIM    = 2                                # up to β2\n",
    "N_JOBS     = 4                                # parallel PH cores\n",
    "# --------------------------------\n",
    "cols = [TIME_COL, SRC, DST, WEIGHT_COL, LABEL,\"packets_count\", \"duration\",\"bytes_rate\",\"syn_flag_counts\"]\n",
    "\n",
    "# Read CSV with specified columns, using object dtype for payload size\n",
    "ddf = dd.read_csv(\n",
    "    CSV_PATH,\n",
    "    usecols=cols,\n",
    "    assume_missing=True,\n",
    "    dtype={WEIGHT_COL: 'object'}\n",
    ")\n",
    "\n",
    "# Manually parse datetime\n",
    "ddf[TIME_COL] = dd.to_datetime(ddf[TIME_COL], errors='coerce')\n",
    "\n",
    "# Set timestamp as index with sorting to ensure global ordering\n",
    "ddf = ddf.set_index(TIME_COL, sorted=False)\n",
    "\n",
    "# Save as sorted CSV\n",
    "ddf.to_csv(\"BFXSS_sorted.csv\", index=True, single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6cbfb",
   "metadata": {},
   "source": [
    "## 1. Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944e5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "WEIGHT_COL = \"total_payload_bytes\"\n",
    "SRC, DST = \"src_ip\", \"dst_ip\"\n",
    "VR_EPS = 2000\n",
    "\n",
    "# Load sorted CSV\n",
    "sorted_ddf = pd.read_csv(\"BFXSS_sorted.csv\", parse_dates=[\"timestamp\"])\n",
    "\n",
    "# Floor timestamps\n",
    "sorted_ddf[\"window_start\"] = sorted_ddf[\"timestamp\"].dt.floor(\"5min\")\n",
    "\n",
    "# # Define start and end times\n",
    "# start_time = pd.Timestamp(\"2018-02-23 12:30:00\")\n",
    "# end_time = pd.Timestamp(\"2018-02-23 12:40:00\")\n",
    "\n",
    "# # Filter\n",
    "# anomaly_window_df = sorted_ddf[(sorted_ddf[\"timestamp\"] >= start_time) & (sorted_ddf[\"timestamp\"] < end_time)]\n",
    "# # We'll skip grouping by 'window_start' since we have a single window now\n",
    "# window_df = anomaly_window_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65211393",
   "metadata": {},
   "source": [
    "## 2. Construct graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f40c8d",
   "metadata": {},
   "source": [
    "Builds a NetworkX graph from an edge list and node features.\n",
    "    \n",
    "    :param edge_list: List of (src, dst, edge_attr_dict)\n",
    "    :param node_features: Dict mapping node ID to feature vector\n",
    "    :return: A NetworkX graph with nodes and edges added\n",
    "\n",
    "Example:\n",
    "Node features: {'IP1': [1.2, 3.4], 'IP2': [0.7, 1.5]}\n",
    "Edge list: [('IP1', 'IP2', {'payload_size': 1024, 'duration': 5})]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e60704",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e51622",
   "metadata": {},
   "source": [
    "build_graph function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "def build_graph(edge_list, node_features):\n",
    "    \"\"\"\n",
    "    Builds a NetworkX graph from an edge list and node features.\n",
    "    \n",
    "    :param edge_list: List of (src, dst, edge_attr_dict)\n",
    "    :param node_features: Dict mapping node ID to feature vector\n",
    "    :return: A NetworkX graph with nodes and edges added\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    for node_id, features in node_features.items():\n",
    "        G.add_node(node_id, features=np.array(features))\n",
    "    for src, dst, edge_attrs in edge_list:\n",
    "        G.add_edge(src, dst, **edge_attrs)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a09b82",
   "metadata": {},
   "source": [
    "calculate edges + make label dictionary (benign/attack) for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b13ba41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate edge feature columns (pick meaningful subset)\n",
    "EDGE_FEATURE_CANDIDATES = [\n",
    "    \"total_payload_bytes\",    # always keep\n",
    "    \"packets_count\",          # basic packet count\n",
    "    \"duration\",               # flow duration\n",
    "    \"syn_flag_counts\"         # simple TCP control feature\n",
    "]\n",
    "\n",
    "# Keep only those that exist in the dataframe\n",
    "EDGE_FEATURE_COLS = [c for c in EDGE_FEATURE_CANDIDATES if c in sorted_ddf.columns]\n",
    "\n",
    "LABEL_MAPPING = {\n",
    "    \"Benign\": 0,\n",
    "    \"Brute_Force_XSS\": 1\n",
    "}\n",
    "\n",
    "# Graph construction\n",
    "graphs = []\n",
    "\n",
    "for window_start, window_df in sorted_ddf.groupby(\"window_start\", sort=False):\n",
    "    edge_list = []\n",
    "\n",
    "    # Build edge list with rich feature vectors\n",
    "    for _, row in window_df.iterrows():\n",
    "        vol = row[WEIGHT_COL]\n",
    "        if pd.isna(vol) or vol <= 0:\n",
    "            continue\n",
    "\n",
    "        edge_attrs = {feat: float(row[feat]) for feat in EDGE_FEATURE_COLS}\n",
    "        edge_attrs[\"weight\"] = min(float(vol), VR_EPS)\n",
    "\n",
    "        edge_list.append((row[SRC], row[DST], edge_attrs))\n",
    "\n",
    "    if not edge_list:\n",
    "        continue\n",
    "\n",
    "    # create label dictionary for model training\n",
    "    label_dict = {}\n",
    "    for _, row in window_df.iterrows():\n",
    "        src = row[\"src_ip\"]\n",
    "        dst = row[\"dst_ip\"]\n",
    "        label_str = row[\"label\"]\n",
    "        label_num = LABEL_MAPPING.get(label_str, 0)  # Default to 0 if missing\n",
    "        edge_key = (src, dst)\n",
    "        if edge_key not in label_dict:\n",
    "            label_dict[edge_key] = label_num\n",
    "        else:\n",
    "            label_dict[edge_key] = max(label_dict[edge_key], label_num)\n",
    "\n",
    "    # Build temporary graph to compute node-level features\n",
    "    G_temp = nx.Graph()\n",
    "    G_temp.add_edges_from([(s, d, {\"weight\": a[\"weight\"]}) for s, d, a in edge_list])\n",
    "\n",
    "    node_features = {}\n",
    "    for node in G_temp.nodes:\n",
    "        degree = G_temp.degree(node)\n",
    "        strength = sum(d[\"weight\"] for _, _, d in G_temp.edges(node, data=True))\n",
    "        node_features[node] = [degree, strength]\n",
    "\n",
    "    # Build final graph\n",
    "    G = build_graph(edge_list, node_features)\n",
    "    graphs.append((window_start, G))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f067699",
   "metadata": {},
   "source": [
    "## Persistent Homology (PH) optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ca73f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gudhi as gd\n",
    "\n",
    "def apply_persistent_homology_optimization(G, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Applies persistent homology optimization on edge attributes.\n",
    "    \n",
    "    :param G: NetworkX graph\n",
    "    :param alpha: Weight for self-edge attribute\n",
    "    :return: None (updates graph edge attributes in-place)\n",
    "    \"\"\"\n",
    "    # Step 1: Collect edge features as points in feature space\n",
    "    edge_points = []\n",
    "    edge_mapping = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        feat = np.array([data.get('payload_size', 0), data.get('duration', 0)])\n",
    "        edge_points.append(feat)\n",
    "        edge_mapping.append((u, v))\n",
    "    \n",
    "    # Step 2: Build Vietoris-Rips complex\n",
    "    rips = gd.RipsComplex(points=edge_points, max_edge_length=2.0)\n",
    "    simplex_tree = rips.create_simplex_tree(max_dimension=2)\n",
    "    diag = simplex_tree.persistence()\n",
    "    \n",
    "    # Step 3: Identify persistent structures\n",
    "    persistent_edges = set()\n",
    "    for interval in diag:\n",
    "        dim, (birth, death) = interval\n",
    "        if dim == 1 and (death - birth) > 0.5:  # threshold can be adjusted\n",
    "            # Assuming we treat all 1D holes as persistent structures\n",
    "            persistent_edges.update(range(len(edge_points)))\n",
    "    \n",
    "    # Step 4: Update attributes\n",
    "    for idx, (u, v) in enumerate(edge_mapping):\n",
    "        current_feat = np.array([G[u][v].get('payload_size', 0), G[u][v].get('duration', 0)])\n",
    "        if idx in persistent_edges:\n",
    "            persistent_feats = [np.array(edge_points[j]) for j in persistent_edges]\n",
    "            mean_feat = np.mean(persistent_feats, axis=0)\n",
    "            updated_feat = alpha * current_feat + (1 - alpha) * mean_feat\n",
    "            G[u][v]['payload_size'], G[u][v]['duration'] = updated_feat[0], updated_feat[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316ba11",
   "metadata": {},
   "source": [
    "## Explicit edge embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2deb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_weight(u_feat, v_feat):\n",
    "    num = np.dot(u_feat, v_feat)\n",
    "    denom = np.linalg.norm(u_feat) * np.linalg.norm(v_feat) + 1e-8  # avoid division by zero\n",
    "    return num / denom\n",
    "\n",
    "def explicit_edge_embedding(G):\n",
    "    \"\"\"\n",
    "    Computes explicit edge embeddings with neighbor aggregation.\n",
    "    \n",
    "    :param G: NetworkX graph\n",
    "    :return: Dict mapping edge (u,v) to embedding vector\n",
    "    \"\"\"\n",
    "    edge_embeddings = {}\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        current_feat = np.array([data.get('payload_size', 0), data.get('duration', 0)])\n",
    "        neighbor_feats = []\n",
    "        \n",
    "        # Collect neighbor edges\n",
    "        neighbor_edges = set(G.edges(u)).union(set(G.edges(v)))\n",
    "        neighbor_edges.discard((u, v))\n",
    "        neighbor_edges.discard((v, u))\n",
    "        \n",
    "        for nu, nv in neighbor_edges:\n",
    "            n_feat_u = G.nodes[nu]['features']\n",
    "            n_feat_v = G.nodes[nv]['features']\n",
    "            weight = compute_edge_weight(n_feat_u, n_feat_v)\n",
    "            neighbor_feat = np.array([\n",
    "                G[nu][nv].get('payload_size', 0),\n",
    "                G[nu][nv].get('duration', 0)\n",
    "            ])\n",
    "            neighbor_feats.append(weight * neighbor_feat)\n",
    "        \n",
    "        # Aggregate neighbor features\n",
    "        if neighbor_feats:\n",
    "            agg_neighbors = np.mean(neighbor_feats, axis=0)\n",
    "        else:\n",
    "            agg_neighbors = np.zeros_like(current_feat)\n",
    "        \n",
    "        embedding = np.concatenate([current_feat, agg_neighbors])\n",
    "        edge_embeddings[(u, v)] = embedding\n",
    "    return edge_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59aaf56",
   "metadata": {},
   "source": [
    "## Anomaly detection module - PhoGAD reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b5a9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PhoGADClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(PhoGADClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)  # binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def focal_loss(pred, target, gamma=2.0):\n",
    "    \"\"\"\n",
    "    Computes focal loss as in Eq. (12) from the paper.\n",
    "    \n",
    "    :param pred: predicted logits (before softmax)\n",
    "    :param target: ground truth labels\n",
    "    :param gamma: focusing parameter\n",
    "    \"\"\"\n",
    "    logpt = -F.cross_entropy(pred, target, reduction='none')\n",
    "    pt = torch.exp(logpt)\n",
    "    loss = -((1 - pt) ** gamma) * logpt\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bd892",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "765661ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, edge_embeddings, edge_labels, epochs=10, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    X = torch.tensor(edge_embeddings, dtype=torch.float32)\n",
    "    y = torch.tensor(edge_labels, dtype=torch.long)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = focal_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef961cb5",
   "metadata": {},
   "source": [
    "# ! MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6124ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# X_files = sorted(glob.glob(\"edge_data/X_*.npy\"))\n",
    "# y_files = sorted(glob.glob(\"edge_data/y_*.npy\"))\n",
    "\n",
    "# X_all = np.vstack([np.load(f) for f in X_files])\n",
    "# y_all = np.hstack([np.load(f) for f in y_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703fa166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5037\n",
      "Epoch 2, Loss: 1.3879\n",
      "Epoch 3, Loss: 1.2783\n",
      "Epoch 4, Loss: 1.1748\n",
      "Epoch 5, Loss: 1.0774\n",
      "Epoch 6, Loss: 0.9863\n",
      "Epoch 7, Loss: 0.9013\n",
      "Epoch 8, Loss: 0.8225\n",
      "Epoch 9, Loss: 0.7496\n",
      "Epoch 10, Loss: 0.6821\n",
      "Epoch 11, Loss: 0.6195\n",
      "Epoch 12, Loss: 0.5613\n",
      "Epoch 13, Loss: 0.5068\n",
      "Epoch 14, Loss: 0.4554\n",
      "Epoch 15, Loss: 0.4065\n",
      "Epoch 16, Loss: 0.3597\n",
      "Epoch 17, Loss: 0.3144\n",
      "Epoch 18, Loss: 0.2705\n",
      "Epoch 19, Loss: 0.2278\n",
      "Epoch 20, Loss: 0.1868\n"
     ]
    }
   ],
   "source": [
    "# 1. Build graph\n",
    "G = build_graph(edge_list, node_features)\n",
    "\n",
    "# 2. Apply persistent homology optimization\n",
    "apply_persistent_homology_optimization(G)\n",
    "\n",
    "# 3. Compute edge embeddings\n",
    "edge_embeddings_dict = explicit_edge_embedding(G)\n",
    "edge_embeddings = np.array(list(edge_embeddings_dict.values()))\n",
    "edge_labels = []\n",
    "for u, v in edge_embeddings_dict.keys():\n",
    "    if (u, v) in label_dict:\n",
    "        label = label_dict[(u, v)]\n",
    "    elif (v, u) in label_dict:  # flip direction\n",
    "        label = label_dict[(v, u)]\n",
    "    else:\n",
    "        label = 0  # default to normal if label is missing (optional)\n",
    "    edge_labels.append(label)\n",
    "edge_labels = np.array(edge_labels)\n",
    "\n",
    "\n",
    "# 4. Initialize and train classifier\n",
    "input_dim = edge_embeddings.shape[1]\n",
    "model = PhoGADClassifier(input_dim, hidden_dim=16)\n",
    "train_model(model, edge_embeddings, edge_labels, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73c21d",
   "metadata": {},
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e1b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         Benign       1.00      0.91      0.95        11\n",
      "Brute_Force_XSS       0.00      0.00      0.00         0\n",
      "\n",
      "       accuracy                           0.91        11\n",
      "      macro avg       0.50      0.45      0.48        11\n",
      "   weighted avg       1.00      0.91      0.95        11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/c/users/maryc/onedrive/desktop/Quantum Research 2025/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    logits = model(torch.tensor(edge_embeddings, dtype=torch.float32))\n",
    "    predictions = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "print(classification_report(\n",
    "    edge_labels, predictions,\n",
    "    labels=[0, 1],  # specify both classes explicitly\n",
    "    target_names=[\"Benign\", \"Brute_Force_XSS\"]\n",
    "))\n",
    "# print(classification_report(edge_labels, predictions, target_names=[\"Benign\", \"Brute_Force_XSS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8963807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution: [11]\n"
     ]
    }
   ],
   "source": [
    "print(\"Label distribution:\", np.bincount(edge_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74444773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Build graph\n",
    "# G = build_graph(edge_list, node_features)\n",
    "\n",
    "# # 2. Apply persistent homology optimization\n",
    "# apply_persistent_homology_optimization(G)\n",
    "\n",
    "# # 3. Compute edge embeddings\n",
    "# edge_embeddings_dict = explicit_edge_embedding(G)\n",
    "# edge_embeddings = np.array(list(edge_embeddings_dict.values()))\n",
    "# edge_labels = []\n",
    "# for u, v in edge_embeddings_dict.keys():\n",
    "#     if (u, v) in label_dict:\n",
    "#         label = label_dict[(u, v)]\n",
    "#     elif (v, u) in label_dict:  # flip direction\n",
    "#         label = label_dict[(v, u)]\n",
    "#     else:\n",
    "#         label = 0  # default to normal if label is missing (optional)\n",
    "#     edge_labels.append(label)\n",
    "# edge_labels = np.array(edge_labels)\n",
    "\n",
    "\n",
    "# # 4. Initialize and train classifier\n",
    "# input_dim = edge_embeddings.shape[1]\n",
    "# model = PhoGADClassifier(input_dim, hidden_dim=16)\n",
    "# train_model(model, edge_embeddings, edge_labels, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
